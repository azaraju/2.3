Hadoop 1.x Major Components are: HDFS and MapReduce. They are also know as “Two Pillars” of Hadoop 1.x. hadoop works on HDFS and MapReduce

HDFS explanation:
HDFS is a Hadoop Distributed FileSystem, where our BigData is stored using Commodity Hardware. It is designed to work with 
Large DataSets with default block size is 64MB (We can change it as per our Project requirements).
HDFS component is again divided into two sub-components:
Name Node explanation:
Name Node is placed in Master Node. It used to store Meta Data about Data Nodes like “How many blocks are stored in Data Nodes,
Which Data Nodes have data, Slave Node Details, Data Nodes locations, timestamps etc”.
Data Node explanation:
Data Nodes are places in Slave Nodes. It is used to store our Application Actual Data. It stores data in Data Slots of size 64MB
by default.
MapReduce explanation:
MapReduce is a Distributed Data Processing or Batch Processing Programming Model. Like HDFS, MapReduce component also uses Commodity
Hardware to process “High Volume of Variety of Data at High Velocity Rate” in a reliable and fault-tolerant manner.
MapReduce component is again divided into two sub-components:
Resource Manager explanation:
Resource Manager is used to assign MapReduce Tasks to Resource Manager in the Cluster of Nodes. Sometimes, it reassigns same tasks to
other Task Trackers as previous Task Trackers are failed or shutdown scenarios.
Resource Manager maintains all the Task Trackers status like Up/running, Failed, Recovered etc.
Node Manager explanation:
Node Manager executes the Tasks which are assigned by Resource Manager and sends the status of those tasks to Resource Manager.
